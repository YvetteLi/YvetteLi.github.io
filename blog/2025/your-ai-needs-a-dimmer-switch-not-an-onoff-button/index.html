<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p><strong>Why ‘more transparency’ is a trap, and the future of explainable AI is adaptive.</strong></p> <p>Imagine you’re a teacher explaining a math problem to two students. One is meticulous, double-checking every step. The other is a skimmer who gets impatient with long passages and just wants the key example.</p> <p>If you give them the exact same explanation, you’ll lose one of them. Guaranteed.</p> <p>That’s the paradox of explainable AI (XAI) today. We build these incredibly sophisticated systems, yet we design their explanations as if every single user thinks and learns in the exact same way. But decades of learning science tell us the opposite is true. People differ in their skills, their personalities, and even their tolerance for detail.</p> <p>For AI tutors and educational tools to be genuinely helpful, their explanations can’t be one-size-fits-all. They need to adapt.</p> <h3>The Transparency Trap: When More Information Backfires</h3> <p>For years, the goal in XAI has been “more transparency.” The thinking was simple: show more of the model’s inner workings — the feature weights, the attention scores — and users will understand.</p> <p>But a <a href="https://dl.acm.org/doi/10.1145/3411764.3445311" rel="external nofollow noopener" target="_blank"><strong>landmark 2021 study</strong></a> involving nearly 3,800 people found that this isn’t true. Making a model more “transparent” didn’t actually help people make better decisions with its help. In fact, the flood of extra detail sometimes distracted them, making them <em>worse</em> at spotting the AI’s mistakes.</p> <p>Cognitive psychology has a term for this: the <a href="https://www.google.com/search?q=https://psycnet.apa.org/record/2002-01479-004&amp;authuser=2" rel="external nofollow noopener" target="_blank"><strong>illusion of explanatory depth</strong></a>. It’s the feeling that you understand something complex (like a bicycle), right up until someone asks you to explain precisely how it works. A superficial, “transparent” explanation can actually reinforce this illusion, giving us a false sense of confidence.</p> <p>Research from the University of British Columbia’s Cristina Conati has shown this in practice. In <a href="https://www.google.com/search?q=https://dl.acm.org/doi/abs/10.1145/3474779.3474797&amp;authuser=2" rel="external nofollow noopener" target="_blank"><strong>one study</strong></a>, her team found that while explanations in an AI tutor increased trust, their effect on learning was messy. Students with lower conscientiousness, for instance, benefited more from the explanations than their meticulous peers. In a <a href="https://www.google.com/search?q=https://dl.acm.org/doi/abs/10.1145/3627706.3653139&amp;authuser=2" rel="external nofollow noopener" target="_blank"><strong>2024 follow-up</strong></a>, they found that learners with higher reading proficiency could handle verbose explanations, while others learned better from concise ones.</p> <p>The message is clear: <strong>more transparency isn’t the answer. Right-sized transparency is.</strong></p> <h3>The Solution: An Explanation Dimmer Switch</h3> <p>So, what if we treated explanations like a dimmer switch instead of an on/off button?</p> <p>An adaptive AI could adjust the “density” of its explanations based on who is asking and what they need in the moment.</p> <ul> <li> <strong>Low Density:</strong> A quick summary. A highlight on a graph. The single most important takeaway.</li> <li> <strong>Medium Density:</strong> Adds the “why.” A short rationale, like <em>“Because you struggled with Concept A, which is a prerequisite for B, we suggest reviewing it.”</em> </li> <li> <strong>High Density:</strong> The full, step-by-step trace, with links to the evidence for each step.</li> </ul> <p>A system could learn to adjust this density automatically based on signals of cognitive load, like a user’s error rate, the time they spend on a task, or even a simple feedback slider.</p> <p>This is what great teachers do instinctively. They sense when a student is overwhelmed and simplify; they know when to challenge another with more depth. An adaptive explanation system could finally do the same.</p> <h3>It’s Not Just What You Say, But How You Show It</h3> <p>Even with the right amount of detail, the design of an explanation matters. A chart or a diagram doesn’t just display information; it <strong>nudges you to think in a certain way.</strong> A bar chart invites you to compare rankings. A flow chart invites you to follow a sequence. This idea of <a href="https://www.google.com/search?q=https://ieeexplore.ieee.org/abstract/document/9623223/&amp;authuser=2" rel="external nofollow noopener" target="_blank"><strong>cognitive affordances</strong></a> is crucial for design.</p> <p>We can apply this to XAI design to encourage specific kinds of thinking:</p> <ul> <li> <strong>To encourage planning,</strong> show a sequential path.</li> <li> <strong>To encourage reflection,</strong> show a “what if” scenario.</li> <li> <strong>To encourage monitoring,</strong> show a high-level summary.</li> </ul> <p>Furthermore, explanations should push learners from being passive observers to active participants. The <a href="https://www.google.com/search?q=https://www.tandfonline.com/doi/abs/10.1080/00461520.2014.916826&amp;authuser=2" rel="external nofollow noopener" target="_blank"><strong>ICAP framework on student engagement</strong></a> shows that learning skyrockets when students move from simply <strong>P</strong>assive reading to being <strong>A</strong>ctive (highlighting), <strong>C</strong>onstructive (summarizing in their own words), or <strong>I</strong>nteractive (debating the point). An explanation shouldn’t be a dead end; it should be a prompt. Instead of a statement, it could be a question: <em>“Can you justify why this step is correct?”</em></p> <h3>The Future Isn’t Just Transparent — It’s Personal</h3> <p>A world where every person gets the same static explanation from an AI is like a classroom where every student is handed the same textbook page, regardless of their needs. We can, and must, do better.</p> <p>By blending principles from human-AI interaction, educational psychology, and user-centered design, we can build systems that finally get this right.</p> <p>The future of explainability isn’t just about creating systems that are transparent. It’s about creating systems that know how to explain themselves <strong>differently to every single one of us.</strong></p> <h3>References</h3> <ol> <li>Bahel, V., Sriram, A., &amp; Conati, C. (2024). Investigating the Interplay Between Learner Characteristics and Explanation Styles in AI-Enhanced Learning. <em>Proceedings of the 29th International Conference on Intelligent User Interfaces (IUI ‘24)</em>.</li> <li>Chi, M. T., &amp; Wylie, R. (2014). The ICAP Framework: Linking Cognitive Engagement to Active Learning Outcomes. <em>Educational Psychologist, 49</em>(4), 219–243.</li> <li>Conati, C., &amp; Kardan, S. (2021). On the Necessity of User-Adaptive Explanations for Intelligent Tutoring Systems. <em>UMAP ’21: Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization</em>.</li> <li>Liu, Z., Padir, T., &amp; Shah, J. A. (2021). Cognitive Affordances of Visualizations for Human-AI Teaming. <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>.</li> <li>Mueller, S. T., Hoffman, R. R., et al. (2021). Principles of Explanation in Human-AI Systems. <em>2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21)</em>.</li> <li>Poursabzi-Sangdeh, F., Goldstein, D. G., et al. (2021). Manipulating and Measuring Model Interpretability. <em>CHI ’21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>.</li> <li>Rozenblit, L., &amp; Keil, F. (2002). The misunderstood limits of folk science: An illusion of explanatory depth. <em>Cognitive Science, 26</em>(5), 521–562.</li> </ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0134c9101f27" width="1" height="1" alt=""></p> </body></html>