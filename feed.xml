<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yvetteli.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yvetteli.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-03T18:36:04+00:00</updated><id>https://yvetteli.github.io/feed.xml</id><title type="html">blank</title><subtitle>Prospective PhD student focused on graph learning and explainable AI for intelligent tutoring— designing learner-facing explanations that support self-regulated learning and equitable access. Current interests: explanation robustness (e.g., deletion curves, counterfactuals), user-centered rationale/alt-text, and graph-based SRL tooling. </subtitle><entry><title type="html">Your AI Needs a Dimmer Switch, Not an On/Off Button</title><link href="https://yvetteli.github.io/blog/2025/your-ai-needs-a-dimmer-switch-not-an-onoff-button/" rel="alternate" type="text/html" title="Your AI Needs a Dimmer Switch, Not an On/Off Button"/><published>2025-10-03T05:02:28+00:00</published><updated>2025-10-03T05:02:28+00:00</updated><id>https://yvetteli.github.io/blog/2025/your-ai-needs-a-dimmer-switch-not-an-onoff-button</id><content type="html" xml:base="https://yvetteli.github.io/blog/2025/your-ai-needs-a-dimmer-switch-not-an-onoff-button/"><![CDATA[<p><strong>Why ‘more transparency’ is a trap, and the future of explainable AI is adaptive.</strong></p> <p>Imagine you’re a teacher explaining a math problem to two students. One is meticulous, double-checking every step. The other is a skimmer who gets impatient with long passages and just wants the key example.</p> <p>If you give them the exact same explanation, you’ll lose one of them. Guaranteed.</p> <p>That’s the paradox of explainable AI (XAI) today. We build these incredibly sophisticated systems, yet we design their explanations as if every single user thinks and learns in the exact same way. But decades of learning science tell us the opposite is true. People differ in their skills, their personalities, and even their tolerance for detail.</p> <p>For AI tutors and educational tools to be genuinely helpful, their explanations can’t be one-size-fits-all. They need to adapt.</p> <h3>The Transparency Trap: When More Information Backfires</h3> <p>For years, the goal in XAI has been “more transparency.” The thinking was simple: show more of the model’s inner workings — the feature weights, the attention scores — and users will understand.</p> <p>But a <a href="https://dl.acm.org/doi/10.1145/3411764.3445311"><strong>landmark 2021 study</strong></a> involving nearly 3,800 people found that this isn’t true. Making a model more “transparent” didn’t actually help people make better decisions with its help. In fact, the flood of extra detail sometimes distracted them, making them <em>worse</em> at spotting the AI’s mistakes.</p> <p>Cognitive psychology has a term for this: the <a href="https://www.google.com/search?q=https://psycnet.apa.org/record/2002-01479-004&amp;authuser=2"><strong>illusion of explanatory depth</strong></a>. It’s the feeling that you understand something complex (like a bicycle), right up until someone asks you to explain precisely how it works. A superficial, “transparent” explanation can actually reinforce this illusion, giving us a false sense of confidence.</p> <p>Research from the University of British Columbia’s Cristina Conati has shown this in practice. In <a href="https://www.google.com/search?q=https://dl.acm.org/doi/abs/10.1145/3474779.3474797&amp;authuser=2"><strong>one study</strong></a>, her team found that while explanations in an AI tutor increased trust, their effect on learning was messy. Students with lower conscientiousness, for instance, benefited more from the explanations than their meticulous peers. In a <a href="https://www.google.com/search?q=https://dl.acm.org/doi/abs/10.1145/3627706.3653139&amp;authuser=2"><strong>2024 follow-up</strong></a>, they found that learners with higher reading proficiency could handle verbose explanations, while others learned better from concise ones.</p> <p>The message is clear: <strong>more transparency isn’t the answer. Right-sized transparency is.</strong></p> <h3>The Solution: An Explanation Dimmer Switch</h3> <p>So, what if we treated explanations like a dimmer switch instead of an on/off button?</p> <p>An adaptive AI could adjust the “density” of its explanations based on who is asking and what they need in the moment.</p> <ul><li><strong>Low Density:</strong> A quick summary. A highlight on a graph. The single most important takeaway.</li><li><strong>Medium Density:</strong> Adds the “why.” A short rationale, like <em>“Because you struggled with Concept A, which is a prerequisite for B, we suggest reviewing it.”</em></li><li><strong>High Density:</strong> The full, step-by-step trace, with links to the evidence for each step.</li></ul> <p>A system could learn to adjust this density automatically based on signals of cognitive load, like a user’s error rate, the time they spend on a task, or even a simple feedback slider.</p> <p>This is what great teachers do instinctively. They sense when a student is overwhelmed and simplify; they know when to challenge another with more depth. An adaptive explanation system could finally do the same.</p> <h3>It’s Not Just What You Say, But How You Show It</h3> <p>Even with the right amount of detail, the design of an explanation matters. A chart or a diagram doesn’t just display information; it <strong>nudges you to think in a certain way.</strong> A bar chart invites you to compare rankings. A flow chart invites you to follow a sequence. This idea of <a href="https://www.google.com/search?q=https://ieeexplore.ieee.org/abstract/document/9623223/&amp;authuser=2"><strong>cognitive affordances</strong></a> is crucial for design.</p> <p>We can apply this to XAI design to encourage specific kinds of thinking:</p> <ul><li><strong>To encourage planning,</strong> show a sequential path.</li><li><strong>To encourage reflection,</strong> show a “what if” scenario.</li><li><strong>To encourage monitoring,</strong> show a high-level summary.</li></ul> <p>Furthermore, explanations should push learners from being passive observers to active participants. The <a href="https://www.google.com/search?q=https://www.tandfonline.com/doi/abs/10.1080/00461520.2014.916826&amp;authuser=2"><strong>ICAP framework on student engagement</strong></a> shows that learning skyrockets when students move from simply <strong>P</strong>assive reading to being <strong>A</strong>ctive (highlighting), <strong>C</strong>onstructive (summarizing in their own words), or <strong>I</strong>nteractive (debating the point). An explanation shouldn’t be a dead end; it should be a prompt. Instead of a statement, it could be a question: <em>“Can you justify why this step is correct?”</em></p> <h3>The Future Isn’t Just Transparent — It’s Personal</h3> <p>A world where every person gets the same static explanation from an AI is like a classroom where every student is handed the same textbook page, regardless of their needs. We can, and must, do better.</p> <p>By blending principles from human-AI interaction, educational psychology, and user-centered design, we can build systems that finally get this right.</p> <p>The future of explainability isn’t just about creating systems that are transparent. It’s about creating systems that know how to explain themselves <strong>differently to every single one of us.</strong></p> <h3>References</h3> <ol><li>Bahel, V., Sriram, A., &amp; Conati, C. (2024). Investigating the Interplay Between Learner Characteristics and Explanation Styles in AI-Enhanced Learning. <em>Proceedings of the 29th International Conference on Intelligent User Interfaces (IUI ‘24)</em>.</li><li>Chi, M. T., &amp; Wylie, R. (2014). The ICAP Framework: Linking Cognitive Engagement to Active Learning Outcomes. <em>Educational Psychologist, 49</em>(4), 219–243.</li><li>Conati, C., &amp; Kardan, S. (2021). On the Necessity of User-Adaptive Explanations for Intelligent Tutoring Systems. <em>UMAP ’21: Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization</em>.</li><li>Liu, Z., Padir, T., &amp; Shah, J. A. (2021). Cognitive Affordances of Visualizations for Human-AI Teaming. <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>.</li><li>Mueller, S. T., Hoffman, R. R., et al. (2021). Principles of Explanation in Human-AI Systems. <em>2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21)</em>.</li><li>Poursabzi-Sangdeh, F., Goldstein, D. G., et al. (2021). Manipulating and Measuring Model Interpretability. <em>CHI ’21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>.</li><li>Rozenblit, L., &amp; Keil, F. (2002). The misunderstood limits of folk science: An illusion of explanatory depth. <em>Cognitive Science, 26</em>(5), 521–562.</li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0134c9101f27" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Your GPS Shows You the Turns. Why Doesn’t Your AI?</title><link href="https://yvetteli.github.io/blog/2025/your-gps-shows-you-the-turns-why-doesnt-your-ai/" rel="alternate" type="text/html" title="Your GPS Shows You the Turns. Why Doesn’t Your AI?"/><published>2025-10-03T04:41:33+00:00</published><updated>2025-10-03T04:41:33+00:00</updated><id>https://yvetteli.github.io/blog/2025/your-gps-shows-you-the-turns-why-doesnt-your-ai</id><content type="html" xml:base="https://yvetteli.github.io/blog/2025/your-gps-shows-you-the-turns-why-doesnt-your-ai/"><![CDATA[<p><strong>Revealing the journey is the true future of explainable AI.</strong></p> <p>Imagine a GPS that only gives you your final destination but none of the turns to get there. You’d end up lost, constantly second-guessing your route, or overshooting it entirely.</p> <p>Many of today’s “explainable” AI systems work this way. They highlight an “important feature” or provide a confidence score, effectively skipping the crucial reasoning path that led to the conclusion. For a student trying to grasp a math solution or a doctor deciding on a treatment, that missing path is the difference between genuine understanding and blind faith.</p> <p>This isn’t a new problem. Education research has been telling us for decades that knowledge doesn’t live in isolated facts — it lives in the connections between them. How we see those connections determines how well we learn.</p> <h3>We’ve Known This for Decades: Learning is a Journey</h3> <p>Back in the early 2000s, a <a href="https://www.google.com/search?q=https://doi.org/10.3102/00346543076004453&amp;authuser=2"><strong>massive meta-analysis by Nesbit &amp; Adesope</strong></a> looked at 142 studies on concept mapping. Their conclusion was crystal clear: learners who <em>built</em> the maps themselves consistently outperformed those who just studied a pre-made one. The act of creating the path fosters a deeper understanding than just seeing the final picture.</p> <p>A decade later, <a href="https://www.google.com/search?q=https://doi.org/10.1002/tea.21434&amp;authuser=2"><strong>another study</strong></a> confirmed the same pattern, finding that novices, in particular, benefit the most from this process. Building a map forces you to think about how concepts connect, emphasizing the routes over memorizing landmarks.</p> <p>Cognitive psychologists call this the <a href="https://www.google.com/search?q=https://doi.org/10.1207/s15516709cog1801_1&amp;authuser=2"><strong>“representational effect.”</strong></a> The way we externalize information — as a list of text versus a visual graph — fundamentally changes how we think. When learners see a problem as a graph, they start noticing constraints, dependencies, and pathways they would have missed in a simple paragraph. Humans simply need to see the path.</p> <h3>AI Is Finally Catching Up</h3> <p>AI research is beginning to embrace what educators have known all along: showing your work is essential.</p> <p><a href="https://arxiv.org/abs/1905.05102"><strong>Models like CogQA</strong></a>, designed for complex question-answering, started building explicit “cognitive graphs” to find answers. Rather than searching a sea of text, the AI constructed a map with entities as nodes and relationships as edges. It then navigated this map to connect the dots across multiple documents. The results were twofold: performance skyrocketed, and its reasoning process became visible as a clear path.</p> <p>This same shift is now happening in Graph Neural Networks (GNNs), a powerful type of AI for understanding networks.</p> <ul><li><a href="https://arxiv.org/abs/2012.10032"><strong>SubgraphX</strong></a> challenged the old method of highlighting data points by identifying compact <em>subgraphs</em> — small, meaningful clusters of nodes and connections — as the explanation. The insight was that we find meaning in structures.</li><li><a href="https://arxiv.org/abs/2305.10714"><strong>PaGE-Link</strong></a> took this even further. When predicting a connection, it presents the <em>paths</em> of existing connections that make a new link likely. In user studies, people overwhelmingly preferred these path-based explanations and trusted the AI more.</li></ul> <p>The message is convergent: <strong>the path is the explanation.</strong></p> <h3>From Confusing Dashboards to Clear Paths</h3> <p>If you’ve ever used an educational dashboard, you’ve seen the charts: bar graphs of “progress” or pie charts of “skills mastered.” They tell you <em>what</em> happened but rarely <em>why</em>. They’re destinations without turns.</p> <p>What would a path-based dashboard look like?</p> <p><strong>A Typical Dashboard Might Say:</strong></p> <blockquote><em>“You scored 65% on derivatives.”</em></blockquote> <p><strong>A Path-Based Dashboard Could Show:</strong></p> <blockquote><em>A path card: “Your path was </em><em>Limits → </em><em>Derivatives → </em><em>Chain Rule. It looks like the confusion started at the </em><em>Limits concept, which is a key prerequisite for the next step.”</em></blockquote> <p>This approach ties directly into self-regulated learning (SRL). It’s a cycle: you plan your path, monitor your progress along it, and reflect on whether you need to change course. A well-designed AI tool evolves from a simple tracker into a navigator that helps you see the road ahead.</p> <h3>Why This Matters: The Map Shapes the Mind</h3> <p>How information is presented changes what we take away from it. A graph triggers conversations about relationships, while a list encourages memorization. In explainable AI, this means we can intentionally design interfaces that highlight prerequisite paths, compare alternative routes, or show the shortest path to mastery.</p> <p>This isn’t just a theoretical vision. At the University of Victoria, <a href="https://www.google.com/search?q=https://www.uvic.ca/ecs/computerscience/people/faculty/s-z/shengyao-lu.php&amp;authuser=2"><strong>Dr. Shengyao Lu and his colleagues</strong></a> are at the forefront of graph explainability, developing methods that move the field away from simplistic scores and toward rich, path-based reasoning. Bringing these advances into our educational and professional tools could be transformative.</p> <p>Imagine an AI tutor that gives you the next problem while also showing you the logical path connecting it to your past work and future goals. That’s the bridge between cutting-edge AI and timeless human psychology.</p> <p>Because in learning, as in life, we trust the path, not just the destination. Explainability should show us the route.</p> <h3>References</h3> <ol><li>Ding, M., Li, C., et al. (2019). <em>CogQA: A Computational Framework for Cognitive Question Answering</em>. arXiv:1905.05102.</li><li>Nesbit, J. C., &amp; Adesope, O. O. (2006). <em>Learning with concept and knowledge maps: A meta-analysis</em>. Review of Educational Research, 76(3), 413–448.</li><li>Schroeder, N. L., Nesbit, J. C., et al. (2018). <em>A meta-analysis of the effects of teaching and learning with concept maps on student science achievement</em>. Journal of Research in Science Teaching, 55(6), 846–871.</li><li>Yuan, H., Yu, H., et al. (2021). <em>On Explainability of Graph Neural Networks via Subgraph Explorations</em>. Proceedings of the 38th International Conference on Machine Learning (ICML).</li><li>Zhang, J., &amp; Norman, D. A. (1994). <em>Representations in distributed cognitive tasks</em>. Cognitive Science, 18(1), 87–122.</li><li>Zhang, Y., Liu, X., et al. (2023). <em>Path-based Explanations for Graph Neural Networks using Path Generation</em>. arXiv:2305.10714.</li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=056df54e4b5b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">When AI Explains Everything, Do We Still Need Concept Maps?</title><link href="https://yvetteli.github.io/blog/2025/when-ai-explains-everything-do-we-still-need-concept-maps/" rel="alternate" type="text/html" title="When AI Explains Everything, Do We Still Need Concept Maps?"/><published>2025-10-03T04:14:04+00:00</published><updated>2025-10-03T04:14:04+00:00</updated><id>https://yvetteli.github.io/blog/2025/when-ai-explains-everything-do-we-still-need-concept-maps</id><content type="html" xml:base="https://yvetteli.github.io/blog/2025/when-ai-explains-everything-do-we-still-need-concept-maps/"><![CDATA[<p><em>Why graphs still matter for metacognition, self-regulation, and explanation UX in the LLM era.</em></p> <p><strong>Why graphs still matter for metacognition, self-regulation, and explanation UX in the LLM era.</strong></p> <p>When a large language model (LLM) can draft an essay or summarise a textbook chapter in seconds, it’s tempting to think the era of “wrestling with ideas” is over. Why struggle to organise your understanding when you can just ask an AI to explain it? Yet decades of research in metacognition and self-regulated learning (SRL) suggest the opposite: if we outsource reflection to automation, we risk losing the very skill that makes learning transfer and judgment possible. This piece argues that concept maps — those humble node-and-link diagrams — are not relics of a pre-AI age. They’re a scaffold for thinking that’s more relevant than ever.</p> <h3>Transparent Explanations Aren’t Enough</h3> <p>Back in 1979, <a href="https://www.google.com/search?q=https://www.semanticscholar.org/paper/Metacognition-and-cognitive-monitoring%253A-A-new-area-Flavell/5443557827ad2f3a61d2165155a80521e78018d9&amp;authuser=2"><strong>John Flavell named the thing we’re talking about: metacognition</strong></a> — planning, monitoring, and evaluating one’s own thinking. These processes are the backbone of SRL and of any durable learning habit.</p> <p>But simply showing people how a system works rarely changes behaviour on its own. A <a href="https://www.google.com/search?q=https://files.eric.ed.gov/fulltext/EJ784458.pdf&amp;authuser=2"><strong>well-cited review of instructional explanations</strong></a> finds that explanations, when presented passively, often have only modest effects unless learners actively use strategies. Likewise, a series of <a href="https://www.google.com/search?q=https://jennwv.com/papers/manipulating_measuring_interpretability_chi_2021.pdf&amp;authuser=2"><strong>large preregistered experiments (~3,800 participants)</strong></a> showed that making a model “more interpretable” can help people simulate it — yet does not reliably improve decision quality, and can even overload them.</p> <p>What does move the needle? Embedding metacognitive support into the doing — e.g., <a href="https://www.google.com/search?q=https://www.tandfonline.com/doi/abs/10.1080/00461520.2012.746453&amp;authuser=2"><strong>prompts that nudge planning and self-monitoring</strong></a> inside tutoring systems — produces robust learning in vivo.</p> <h3>Concept Maps as Metacognitive Tools (Not Just Pretty Diagrams)</h3> <p>A concept map is a graph of nodes (concepts) and labelled links (relations). <a href="https://www.google.com/search?q=https://www.researchgate.net/publication/220427382_Learning_with_concept_and_knowledge_maps_A_meta-analysis&amp;authuser=2"><strong>Meta-analyses across dozens of experiments</strong></a> report moderate learning gains when learners construct or study maps, with bigger effects when learners build the map themselves. That “make your own” bit matters: the act of structuring knowledge is the intervention.</p> <p>Maps also offload cognition: by distributing information across a visual structure, they free working memory for reflection and sense-making. This aligns with classic results in <a href="https://www.google.com/search?q=https://pages.ucsd.edu/~jzhang/publications/Zhang-Norman-Cognitive-Science-1994.pdf&amp;authuser=2"><strong>distributed cognition and the representational effect</strong></a> — isomorphic information, arranged differently, can lead to very different thinking performance.</p> <h3>Why Maps Matter More with LLMs</h3> <p>Two cognitive pitfalls are amplified in the LLM era:</p> <ol><li><strong>Illusion of explanatory depth (IOED).</strong> People often believe they understand a mechanism <a href="https://www.google.com/search?q=https://time.com/5316497/the-illusion-of-explanatory-depth/&amp;authuser=2"><strong>until they try to explain it</strong></a>. Without an external prompt to articulate relations (“How does A lead to B?”), we overestimate comprehension.</li><li><strong>Over-transparency.</strong> More model details <a href="https://www.google.com/search?q=https://jennwv.com/papers/manipulating_measuring_interpretability_chi_2021.pdf&amp;authuser=2"><strong>do not guarantee better decisions</strong></a>; dumping coefficients or attention heatmaps can swamp users.</li></ol> <p>Concept mapping counters both. When you build a map from an AI explanation, you must extract key ideas, name relations, and confront gaps — a concrete antidote to IOED that redirects attention from fluent text to structured reasoning.</p> <h3>From Static Diagrams to SRL Cycles You Can See</h3> <p>SRL isn’t a fixed trait; it’s a cycle in time: Plan → Monitor/Control → Reflect → (repeat). Concept mapping tasks naturally enact each phase. In learning analytics, researchers have shown how to turn raw user actions into <a href="https://www.google.com/search?q=https://www.learning-analytics.info/journals/index.php/JLA/article/view/4260&amp;authuser=2"><strong>micro-level SRL events</strong></a> to see whether scaffolds actually spark regulation. Mapping actions don’t have to be logged as “drawing time” — they can be coded as evidence of planning, monitoring, or reflection.</p> <h3>The Interface Is Not Neutral</h3> <p>Even when two visuals are “about the same thing,” they afford different inferences. Classic <a href="https://www.google.com/search?q=https://www.jucs.org/jucs_9_6/an_empirical_study_of/Suthers_D.pdf&amp;authuser=2"><strong>representational guidance work</strong></a> shows that tool notation (text vs. graph vs. matrix) measurably changes the kind of discussion and evidence learners produce together.</p> <p>A newer framework translates this to visualization: design choices and reader traits together shape the hierarchy of messages a chart makes most likely — the takeaways it invites. This is a powerful lens for explanation UX: we should design UIs to maximize the intended takeaway for a given learner, not just display everything.</p> <h3>Bottom Line</h3> <p>LLMs are fantastic at producing; humans still need tools for thinking. Concept maps — done right — don’t just re-package content. They instantiate self-regulated learning cycles and provide affordances that steer attention toward relationships and mechanisms. In other words: maps are not a nostalgia play. They’re a modern control surface for metacognition.</p> <h3>References</h3> <ol><li>Azevedo, R., &amp; Aleven, V. (Eds.). (2013). <em>International handbook of metacognition and learning technologies</em>. Springer.</li><li>Flavell, J. H. (1979). Metacognition and cognitive monitoring: A new area of cognitive-developmental inquiry. <em>American Psychologist, 34</em>(10), 906–911.</li><li>Liu, Z., Padir, T., &amp; Shah, J. A. (2021). Cognitive Affordances of Visualizations for Human-AI Teaming. <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>.</li><li>Nesbit, J. C., &amp; Adesope, O. O. (2006). Learning with concept and knowledge maps: A meta-analysis. <em>Review of Educational Research, 76</em>(3), 413–448.</li><li>Poursabzi-Sangdeh, F., Goldstein, D. G., et al. (2021). Manipulating and Measuring Model Interpretability. <em>CHI ’21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>.</li><li>Siadaty, M., Gašević, D., &amp; Hatala, M. (2016). Trace-based analysis of self-regulated learning: a systematic review and a framework. <em>Journal of Learning Analytics, 3</em>(3), 51–84.</li><li>Suthers, D. D., &amp; Hundhausen, C. D. (2003). An empirical study of the effects of representational guidance on collaborative learning. <em>Journal of Universal Computer Science, 9</em>(6), 586–600.</li><li>Wittwer, J., &amp; Renkl, A. (2008). Why Instructional Explanations Often Do Not Work: A Framework for Understanding and Enhancing Their Effectiveness. <em>Educational Psychologist, 43</em>(1), 49–64.</li><li>Zhang, J., &amp; Norman, D. A. (1994). Representations in distributed cognitive tasks. <em>Cognitive Science, 18</em>(1), 87–122.</li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=024f63188def" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>